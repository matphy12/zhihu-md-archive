## 文章：单一架构的缺陷——为什么AI是复杂的工程问题

当前的大语言模型尽管已经展现出惊人的语言理解与生成能力，但仍缺乏一些人类记忆系统天然具备的能力。例如，人类在遗忘某件事时，可能无法直接回忆出事件细节，但却可以通过“回忆当时的过程”逐步重新激活相关信息。这种“情境激活式回忆”是 Transformer 架构所不具备的。Transformer 的 attention 是一次性完成的信息聚合，缺乏路径探索与动态重构能力。

如果说 Transformer 是“语言皮层”的模拟，未来的 AI 架构也许应当从“脑区仿生”出发，模拟不同类型脑区的分工与协作，来完善其当前所不具备的认知功能。因此，智能可能不应依赖于一个“万能模型”，而应是多个机制协作的产物。

事实上，很多已有的生成模型早已在尝试建模“人类回忆”的过程：Hopfield 网络模拟了联想记忆与模式重建；扩散模型则展现了从模糊状态向清晰状态的逐步重构能力。如何把这些模型的不同特性整合进一个 AGI 大脑，是一个值得深入思考的方向。下一代架构或许将是多模块的协同，而非单一 LLM 。

另外，还应注意，人类的运动系统并非完全由文本驱动，肌肉记忆、神经反射与微小扰动在肢体控制中扮演了极其重要的角色。因此，未来机器人的“动作生成”也许不是单纯由语言模型控制，而是语言、肌肉记忆系统与随机扰动系统的协同结果。

总体来看，AGI 的构建不能仅依赖 Scaling Law，而是一项复杂的工程系统问题。我们需要有意识地模仿大脑的结构优势，补足 Transformer 的劣势，但同时也不能无脑仿生，要充分发挥计算机的特有优势——如高效存储、高速搜索、高速计算等能力。

编辑于 2025-07-02 16:26