## 文章：关于嵌入空间

一个自然的问题是：为什么大型语言模型的嵌入空间越大，其推理效率反而会下降？

从直觉上看，一个说话者若备选词语过多，就更难快速选出最合适的表达。语言模型同样如此：虽然经过强化学习，数学专家模型会倾向于使用逻辑与数学相关词汇，而对无关词（如玄学、社交类词汇）赋予极低权重，但这些词依然存在于整个嵌入空间中，从而影响选择与搜索过程的效率。

但是，是否意味着缩小嵌入矩阵就能获得最优的推理效率？结论是当然是否。

因为推理不仅包括基于语言的逻辑推理，如果所有图形都用逻辑符号来定义，这样的效率同样一定是不高的。如果我们强行要求模型仅用一阶逻辑符号来表示所有几何对象，例如用集合论定义圆：

定义实数、定义点集、定义坐标系、定义距离、定义圆方程

那么在圆之间进行推理时，模型将不得不操作高度冗余的逻辑结构，其计算效率必然极低。

因此，我们需要抽象出几何对象的结构本质。例如圆，可以被表述为：

“所有到某中心点距离相等的点的集合”

这是一种结构性的定义，直接抓住了圆的核心不变量。

但这种抽象结构无法直接作为语言模型的“词嵌入”元素，因为嵌入矩阵无法存储“一个新定义”或“一个结构”，它只能存储固定离散的 token。

然而，从推理的角度看，这类结构本质上应该成为推理过程中的“抽象可选项”——它们是结构空间中的原语，而不是语言空间中的 token。